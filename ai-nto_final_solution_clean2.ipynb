{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-02T19:08:12.264423Z",
     "iopub.status.busy": "2025-12-02T19:08:12.264207Z",
     "iopub.status.idle": "2025-12-02T19:08:15.219095Z",
     "shell.execute_reply": "2025-12-02T19:08:15.218189Z",
     "shell.execute_reply.started": "2025-12-02T19:08:12.264403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/eto-ai-nto-suchka/book_genres.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/sample_submission.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/book_descriptions.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/users.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/constants.py\n",
      "/kaggle/input/eto-ai-nto-suchka/genres.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/books.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/config.py\n",
      "/kaggle/input/eto-ai-nto-suchka/submission.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/train.csv\n",
      "/kaggle/input/eto-ai-nto-suchka/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import random\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:08:18.955913Z",
     "iopub.status.busy": "2025-12-02T19:08:18.955368Z",
     "iopub.status.idle": "2025-12-02T19:08:18.964961Z",
     "shell.execute_reply": "2025-12-02T19:08:18.964115Z",
     "shell.execute_reply.started": "2025-12-02T19:08:18.955881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    # 1. Python standard library\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # 2. NumPy (used by Pandas)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 3. PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Call it once at the start\n",
    "seed_everything(SEED)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:08:18.966264Z",
     "iopub.status.busy": "2025-12-02T19:08:18.965927Z",
     "iopub.status.idle": "2025-12-02T19:08:20.449714Z",
     "shell.execute_reply": "2025-12-02T19:08:20.449102Z",
     "shell.execute_reply.started": "2025-12-02T19:08:18.966243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "book_decs = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/book_descriptions.csv')\n",
    "book_genres = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/book_genres.csv')\n",
    "genres = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/genres.csv')\n",
    "books = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/books.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/sample_submission.csv')\n",
    "test = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/test.csv')\n",
    "train = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/train.csv')\n",
    "users = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:08:20.451018Z",
     "iopub.status.busy": "2025-12-02T19:08:20.450688Z",
     "iopub.status.idle": "2025-12-02T19:08:21.308810Z",
     "shell.execute_reply": "2025-12-02T19:08:21.308061Z",
     "shell.execute_reply.started": "2025-12-02T19:08:20.450986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! submission_superblend_conditional_3.0roundedto0.csv saved\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1. LOAD DATA\n",
    "# ============================\n",
    "train = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/test.csv')\n",
    "books = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/books.csv')\n",
    "genres = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/book_genres.csv')\n",
    "\n",
    "train = train[train[\"has_read\"] == 1].copy()\n",
    "global_mean = train[\"rating\"].mean()\n",
    "\n",
    "# ============================\n",
    "# 2. MERGE META\n",
    "# ============================\n",
    "train = train.merge(books[[\"book_id\", \"author_id\"]], on=\"book_id\", how=\"left\")\n",
    "test  = test.merge(books[[\"book_id\", \"author_id\"]], on=\"book_id\", how=\"left\")\n",
    "\n",
    "book_genres_full = genres.copy()\n",
    "\n",
    "# ============================\n",
    "# 2.1 USER STD (для твоего правила)\n",
    "# ============================\n",
    "user_stats_full = (\n",
    "    train.groupby(\"user_id\")[\"rating\"]\n",
    "        .agg([\"mean\", \"std\", \"count\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"mean\": \"user_mean\", \"std\": \"user_std\", \"count\": \"user_count\"})\n",
    ")\n",
    "# std может быть NaN если 1 оценка — считаем шум 0 (или можно big number, но ты просил по std>4)\n",
    "user_stats_full[\"user_std\"] = user_stats_full[\"user_std\"].fillna(0.0)\n",
    "\n",
    "# ============================\n",
    "# 3. EXPAND TRAIN TO MULTI-GENRE\n",
    "# ============================\n",
    "train_expanded = train.merge(book_genres_full, on=\"book_id\", how=\"left\")\n",
    "\n",
    "# ============================\n",
    "# 4. USER–GENRE MEAN & COUNT\n",
    "# ============================\n",
    "user_genre_stats = (\n",
    "    train_expanded.groupby([\"user_id\", \"genre_id\"])[\"rating\"]\n",
    "        .agg([\"mean\", \"count\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"mean\": \"user_genre_mean\", \"count\": \"user_genre_count\"})\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 5. USER–AUTHOR MEAN\n",
    "# ============================\n",
    "user_author_mean = (\n",
    "    train.groupby([\"user_id\", \"author_id\"])[\"rating\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"rating\": \"user_author_mean\"})\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 6. USER MEAN (уже есть в user_stats_full, но оставим для совместимости)\n",
    "# ============================\n",
    "user_mean = user_stats_full[[\"user_id\", \"user_mean\"]].copy()\n",
    "\n",
    "# ============================\n",
    "# 7. BOOK MEAN\n",
    "# ============================\n",
    "book_mean = (\n",
    "    train.groupby(\"book_id\")[\"rating\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"rating\": \"book_mean\"})\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 8. AUTHOR GLOBAL MEAN\n",
    "# ============================\n",
    "author_global_mean = (\n",
    "    train.groupby(\"author_id\")[\"rating\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"rating\": \"author_global_mean\"})\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 9. GENRE GLOBAL MEAN + BOOK GENRE GLOBAL MEAN\n",
    "# ============================\n",
    "genre_global_mean = (\n",
    "    train_expanded.groupby(\"genre_id\")[\"rating\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"rating\": \"genre_global_mean\"})\n",
    ")\n",
    "\n",
    "book_genre_global_mean = (\n",
    "    book_genres_full\n",
    "        .merge(genre_global_mean, on=\"genre_id\", how=\"left\")\n",
    "        .groupby(\"book_id\")[\"genre_global_mean\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"genre_global_mean\": \"book_genre_global_mean\"})\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 10. EXPAND TEST TO MULTI-GENRE\n",
    "# ============================\n",
    "test_multi = test.merge(book_genres_full, on=\"book_id\", how=\"left\")\n",
    "\n",
    "test_multi = test_multi.merge(\n",
    "    user_genre_stats,\n",
    "    on=[\"user_id\", \"genre_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "test_multi[\"user_genre_mean\"]  = test_multi[\"user_genre_mean\"].fillna(global_mean)\n",
    "test_multi[\"user_genre_count\"] = test_multi[\"user_genre_count\"].fillna(0)\n",
    "\n",
    "# ============================\n",
    "# 11. GENRE WEIGHTS\n",
    "# ============================\n",
    "test_multi[\"user_genre_count_smooth\"] = test_multi[\"user_genre_count\"] + 1\n",
    "\n",
    "test_multi[\"user_total_genre_reads\"] = (\n",
    "    test_multi.groupby(\"user_id\")[\"user_genre_count_smooth\"].transform(\"sum\")\n",
    ")\n",
    "\n",
    "test_multi[\"genre_weight\"] = (\n",
    "    test_multi[\"user_genre_count_smooth\"] / test_multi[\"user_total_genre_reads\"]\n",
    ")\n",
    "\n",
    "test_multi[\"normalized_weight\"] = (\n",
    "    test_multi[\"genre_weight\"] /\n",
    "    test_multi.groupby([\"user_id\",\"book_id\"])[\"genre_weight\"].transform(\"sum\")\n",
    ").fillna(0)\n",
    "\n",
    "# ============================\n",
    "# 12. WEIGHTED GENRE MEAN\n",
    "# ============================\n",
    "test_multi[\"weighted_genre_mean\"] = (\n",
    "    test_multi[\"normalized_weight\"] * test_multi[\"user_genre_mean\"]\n",
    ")\n",
    "\n",
    "user_multi_genre_mean = (\n",
    "    test_multi.groupby([\"user_id\", \"book_id\"])[\"weighted_genre_mean\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"weighted_genre_mean\": \"genre_weighted_prediction\"})\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 13. MERGE EVERYTHING В TEST2\n",
    "# ============================\n",
    "test2 = test.merge(user_multi_genre_mean, on=[\"user_id\", \"book_id\"], how=\"left\")\n",
    "test2[\"genre_weighted_prediction\"] = test2[\"genre_weighted_prediction\"].fillna(global_mean)\n",
    "\n",
    "test2 = test2.merge(user_author_mean, on=[\"user_id\", \"author_id\"], how=\"left\")\n",
    "\n",
    "test2 = test2.merge(user_mean, on=\"user_id\", how=\"left\")\n",
    "test2 = test2.merge(user_stats_full[[\"user_id\", \"user_std\"]], on=\"user_id\", how=\"left\")\n",
    "test2[\"user_std\"] = test2[\"user_std\"].fillna(0.0)\n",
    "\n",
    "test2 = test2.merge(book_mean, on=\"book_id\", how=\"left\")\n",
    "\n",
    "test2 = test2.merge(author_global_mean, on=\"author_id\", how=\"left\")\n",
    "test2[\"author_global_mean\"] = test2[\"author_global_mean\"].fillna(global_mean)\n",
    "\n",
    "test2 = test2.merge(book_genre_global_mean, on=\"book_id\", how=\"left\")\n",
    "test2[\"book_genre_global_mean\"] = test2[\"book_genre_global_mean\"].fillna(global_mean)\n",
    "\n",
    "# ============================\n",
    "# 14. FLAGS: known user / known book\n",
    "# ============================\n",
    "known_users = set(train[\"user_id\"].unique())\n",
    "known_books = set(train[\"book_id\"].unique())\n",
    "\n",
    "test2[\"known_user\"] = test2[\"user_id\"].isin(known_users)\n",
    "test2[\"known_book\"] = test2[\"book_id\"].isin(known_books)\n",
    "\n",
    "# ============================\n",
    "# 15. CONDITIONAL PREDICTIONS (WITH YOUR STD>4 RULE)\n",
    "# ============================\n",
    "def weighted_avg(candidates):\n",
    "    vals = [(v, w) for (v, w) in candidates if pd.notna(v)]\n",
    "    if not vals:\n",
    "        return global_mean\n",
    "    ws = np.array([w for _, w in vals], dtype=float)\n",
    "    ws = ws / ws.sum()\n",
    "    xs = np.array([v for v, _ in vals], dtype=float)\n",
    "    return float((xs * ws).sum())\n",
    "\n",
    "def rebalance_user_vs_global(cands, user_keys, global_keys, force_equal=True):\n",
    "    \"\"\"\n",
    "    cands: list of tuples (key, value, weight)\n",
    "    user_keys/global_keys: which keys belong to which group\n",
    "    force_equal: если True -> сумма весов user == сумма весов global (только по доступным сигналам!)\n",
    "    \"\"\"\n",
    "    # оставим только не-NaN\n",
    "    cands = [(k, v, w) for (k, v, w) in cands if pd.notna(v)]\n",
    "    if not cands:\n",
    "        return []\n",
    "\n",
    "    user = [(k, v, w) for (k, v, w) in cands if k in user_keys]\n",
    "    glob = [(k, v, w) for (k, v, w) in cands if k in global_keys]\n",
    "    other = [(k, v, w) for (k, v, w) in cands if (k not in user_keys and k not in global_keys)]\n",
    "\n",
    "    # если одной группы вообще нет — нечего уравнивать\n",
    "    if not user or not glob or not force_equal:\n",
    "        return [(v, w) for (_, v, w) in cands]\n",
    "\n",
    "    sum_u = sum(w for _, _, w in user)\n",
    "    sum_g = sum(w for _, _, w in glob)\n",
    "\n",
    "    # хотим: sum_u_new == sum_g_new == (sum_u + sum_g)/2\n",
    "    target = 0.5 * (sum_u + sum_g)\n",
    "\n",
    "    # масштабирование внутри групп, сохраняя пропорции\n",
    "    scale_u = target / sum_u if sum_u > 0 else 1.0\n",
    "    scale_g = target / sum_g if sum_g > 0 else 1.0\n",
    "\n",
    "    user2 = [(v, w * scale_u) for _, v, w in user]\n",
    "    glob2 = [(v, w * scale_g) for _, v, w in glob]\n",
    "    other2 = [(v, w) for _, v, w in other]\n",
    "\n",
    "    return user2 + glob2 + other2\n",
    "\n",
    "def conditional_pred(row):\n",
    "    ku = row[\"known_user\"]\n",
    "    kb = row[\"known_book\"]\n",
    "    ustd = row[\"user_std\"]\n",
    "\n",
    "    # сигналы\n",
    "    genre_pred    = row[\"genre_weighted_prediction\"]      # число\n",
    "    author_user   = row[\"user_author_mean\"]               # NaN возможно\n",
    "    user_m        = row[\"user_mean\"]                      # NaN возможно\n",
    "    book_m        = row[\"book_mean\"]                      # NaN возможно\n",
    "    author_global = row[\"author_global_mean\"]             # число\n",
    "    genre_global  = row[\"book_genre_global_mean\"]         # число\n",
    "\n",
    "    # какие сигналы \"связанные с юзером\"\n",
    "    USER_KEYS = {\"genre_pred\", \"author_user\", \"user_m\"}\n",
    "    GLOBAL_KEYS = {\"author_global\", \"genre_global\", \"book_m\"}\n",
    "\n",
    "    # --- 1) known user & known book ---\n",
    "    if ku and kb:\n",
    "        base = [\n",
    "            (\"genre_pred\",    genre_pred,    0.30),\n",
    "            (\"author_user\",   author_user,   0.20),\n",
    "            (\"user_m\",        user_m,        0.15),\n",
    "            (\"book_m\",        book_m,        0.15),\n",
    "            (\"author_global\", author_global, 0.10),\n",
    "            (\"genre_global\",  genre_global,  0.10),\n",
    "        ]\n",
    "\n",
    "        # твоя идея: если user_std > 4, сделать user-часть == global-части\n",
    "        if ustd > 4:\n",
    "            cands = rebalance_user_vs_global(base, USER_KEYS, GLOBAL_KEYS, force_equal=True)\n",
    "        else:\n",
    "            cands = [(v, w) for (_, v, w) in base]\n",
    "\n",
    "        return weighted_avg(cands)\n",
    "\n",
    "    # --- 2) known user, new book ---\n",
    "    if ku and not kb:\n",
    "        base = [\n",
    "            (\"genre_pred\",    genre_pred,    0.40),\n",
    "            (\"author_user\",   author_user,   0.35),\n",
    "            (\"user_m\",        user_m,        0.15),\n",
    "            (\"author_global\", author_global, 0.10),\n",
    "            (\"genre_global\",  genre_global,  0.10),\n",
    "        ]\n",
    "\n",
    "        if ustd > 4:\n",
    "            cands = rebalance_user_vs_global(base, USER_KEYS, GLOBAL_KEYS, force_equal=True)\n",
    "        else:\n",
    "            cands = [(v, w) for (_, v, w) in base]\n",
    "\n",
    "        return weighted_avg(cands)\n",
    "\n",
    "    # --- 3) new user, known book ---\n",
    "    if (not ku) and kb:\n",
    "        base = [\n",
    "            (\"book_m\",        book_m,        0.40),\n",
    "            (\"author_global\", author_global, 0.30),\n",
    "            (\"genre_global\",  genre_global,  0.30),\n",
    "        ]\n",
    "        cands = [(v, w) for (_, v, w) in base]\n",
    "        return weighted_avg(cands)\n",
    "\n",
    "    # --- 4) cold start ---\n",
    "    base = [\n",
    "        (\"author_global\", author_global, 0.50),\n",
    "        (\"genre_global\",  genre_global,  0.50),\n",
    "    ]\n",
    "    cands = [(v, w) for (_, v, w) in base]\n",
    "    return weighted_avg(cands)\n",
    "\n",
    "test2[\"rating_predict\"] = test2.apply(conditional_pred, axis=1)\n",
    "test2[\"rating_predict\"] = np.clip(test2[\"rating_predict\"], 0, 10) * 1.03\n",
    "\n",
    "# ============================\n",
    "# 16. SUBMISSION\n",
    "# ============================\n",
    "test2[[\"user_id\",\"book_id\",\"rating_predict\"]].to_csv(\n",
    "    \"submission_superblend_conditional_3.0roundedto0.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\"DONE! submission_superblend_conditional_3.0roundedto0.csv saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:08:21.309792Z",
     "iopub.status.busy": "2025-12-02T19:08:21.309574Z",
     "iopub.status.idle": "2025-12-02T19:08:23.069673Z",
     "shell.execute_reply": "2025-12-02T19:08:23.068927Z",
     "shell.execute_reply.started": "2025-12-02T19:08:21.309775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean: 7.6632\n",
      "Генерация V2...\n",
      "Готово! Пробуй submission_algo_v2.csv\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/test.csv')\n",
    "books = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/books.csv')\n",
    "book_genres = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/book_genres.csv')\n",
    "\n",
    "train = train[train['has_read'] == 1]\n",
    "\n",
    "user_smooth = 10\n",
    "book_smooth = 5\n",
    "genre_smooth = 5\n",
    "\n",
    "average_rating = train['rating'].mean()\n",
    "\n",
    "user_group = train.groupby('user_id')['rating'].agg(['sum', 'count'])\n",
    "user_group['user_adjust'] = (user_group['sum'] + user_smooth * average_rating) / (user_group['count'] + user_smooth) - average_rating\n",
    "user_adj_dict = user_group['user_adjust'].to_dict()\n",
    "\n",
    "train = train.merge(user_group[['user_adjust']], on='user_id', how='left')\n",
    "train['temp_resid'] = train['rating'] - average_rating - train['user_adjust']\n",
    "\n",
    "book_group = train.groupby('book_id')['temp_resid'].agg(['sum', 'count'])\n",
    "book_group['book_adjust'] = book_group['sum'] / (book_group['count'] + book_smooth)\n",
    "book_adj_dict = book_group['book_adjust'].to_dict()\n",
    "\n",
    "train_genres = train[['user_id', 'book_id', 'temp_resid']].merge(book_genres, on='book_id')\n",
    "train_genres['book_adjust_val'] = train_genres['book_id'].map(book_adj_dict).fillna(0)\n",
    "train_genres['genre_resid'] = train_genres['temp_resid'] - train_genres['book_adjust_val']\n",
    "\n",
    "user_genre_group = train_genres.groupby(['user_id', 'genre_id'])['genre_resid'].agg(['sum', 'count'])\n",
    "user_genre_group['genre_adjust'] = user_genre_group['sum'] / (user_genre_group['count'] + genre_smooth)\n",
    "user_genre_dict = user_genre_group['genre_adjust'].to_dict()\n",
    "\n",
    "book_to_genres = book_genres.groupby('book_id')['genre_id'].apply(list).to_dict()\n",
    "\n",
    "def make_prediction(row):\n",
    "    user = row['user_id']\n",
    "    book = row['book_id']\n",
    "    \n",
    "    pred_value = average_rating + user_adj_dict.get(user, 0.0)\n",
    "    pred_value += book_adj_dict.get(book, 0.0)\n",
    "    \n",
    "    genres = book_to_genres.get(book, [])\n",
    "    if genres:\n",
    "        genre_vals = []\n",
    "        for g in genres:\n",
    "            val = user_genre_dict.get((user, g))\n",
    "            if val is not None:\n",
    "                genre_vals.append(val)\n",
    "        if genre_vals:\n",
    "            pred_value += np.mean(genre_vals)\n",
    "            \n",
    "    return np.clip(pred_value, 0, 10)\n",
    "\n",
    "test['rating_predict'] = test.apply(make_prediction, axis=1)\n",
    "\n",
    "submission = test[['user_id', 'book_id', 'rating_predict']]\n",
    "submission.to_csv('submission_algo2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:08:23.070852Z",
     "iopub.status.busy": "2025-12-02T19:08:23.070463Z",
     "iopub.status.idle": "2025-12-02T19:08:23.100372Z",
     "shell.execute_reply": "2025-12-02T19:08:23.099753Z",
     "shell.execute_reply.started": "2025-12-02T19:08:23.070830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>281</td>\n",
       "      <td>2461928</td>\n",
       "      <td>8.381563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1250</td>\n",
       "      <td>31957</td>\n",
       "      <td>6.698717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4241</td>\n",
       "      <td>196603</td>\n",
       "      <td>8.108635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5140</td>\n",
       "      <td>468894</td>\n",
       "      <td>8.487695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7781</td>\n",
       "      <td>2141951</td>\n",
       "      <td>7.523158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating_predict\n",
       "0      281  2461928        8.381563\n",
       "1     1250    31957        6.698717\n",
       "2     4241   196603        8.108635\n",
       "3     5140   468894        8.487695\n",
       "4     7781  2141951        7.523158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_pred = pd.read_csv('/kaggle/input/eto-ai-nto-suchka/submission.csv')\n",
    "bias_pred = pd.read_csv('/kaggle/working/submission_algo2.csv')\n",
    "my_pred = pd.read_csv('/kaggle/working/submission_superblend_conditional_3.0roundedto0.csv')\n",
    "\n",
    "super_mega_blend = pd.DataFrame({\n",
    "    'user_id': base_pred['user_id'],\n",
    "    'book_id': base_pred['book_id'],\n",
    "    'rating_predict': my_pred['rating_predict']*0.70 + bias_pred['rating_predict']*0.175 + base_pred['rating_predict']*0.125})\n",
    "super_mega_blend.to_csv('SUPER_MEGA_BLEND_SIGMA_SOTA_KILLER_GONCHAROV_KIRPICHENKO_KHLOPOTNUKH_SOLUTION676767.csv', index=False)\n",
    "super_mega_blend.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8799236,
     "sourceId": 13817761,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
